{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from functions import *\n",
    "\n",
    "df_final_demo = pd.read_csv(\"df_final_demo.csv\")\n",
    "\n",
    "df_final_demo.head()\n",
    "df_final_demo.dropna(inplace=True)\n",
    "\n",
    "mapGenre = {'X' : 'U'}\n",
    "df_final_demo[\"gendr\"] = replace_values_df(df_final_demo[\"gendr\"], mapGenre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web_data1 = pd.read_csv(\"df_final_web_data_pt_1.csv\")\n",
    "df_web_data2 = pd.read_csv(\"df_final_web_data_pt_2.csv\")\n",
    "\n",
    "df_web_data_concat = pd.concat([df_web_data1, df_web_data2], axis=0, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type object into datetime \n",
    "\n",
    "df_web_data_concat['date_time'] = convert_to_dateTime(df_web_data_concat['date_time'])\n",
    "\n",
    "df_web_data_concat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I turn 'step' into a discrete numeric df\n",
    "\n",
    "map_values = {'start' : 0, 'step_1' : 1, 'step_2' : 2, 'step_3' : 3, 'confirm' : 4}\n",
    "\n",
    "df_web_data_concat[\"process_step\"] = replace_values_df(df_web_data_concat[\"process_step\"], map_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_exp = pd.read_csv(\"df_final_experiment_clients.csv\")\n",
    "df_final_exp.rename(columns={\"Variation\" : \"variation\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_exp = drop_na_df(df_final_exp, \"variation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_values2 = {'Control': 0, 'Test': 1}\n",
    "\n",
    "df_final_exp[\"variation\"] = replace_values_df(df_final_exp[\"variation\"], map_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = pd.merge(df_final_demo, df_web_data_concat, on='client_id', how='inner')\n",
    "\n",
    "df_all = pd.merge(df_temp1, df_final_exp, on='client_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.drop(\"clnt_tenure_yr\", axis=1)      # We don´t need this column because is derivative from clnt_tenure_mnth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_no_duplicates = df_all.copy()\n",
    "df_all_no_duplicates.drop_duplicates(subset=\"client_id\", inplace=True)\n",
    "\n",
    "# El dataframe pero sin los duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUITAMOS OUTLIERS DEL DF_ALL Y DEL QUE NO TIENE DUPLICADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = outlier_slayer(df_all)\n",
    "\n",
    "df_all_no_duplicates = outlier_slayer(df_all_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VER SI SON JOVENES O NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_no_duplicates[\"clnt_age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"clnt_age\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data = df_all_no_duplicates['clnt_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_all_no_duplicates['clnt_age'].quantile(0.25)\n",
    "Q3 = df_all_no_duplicates['clnt_age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "inf_lim = Q1 - 1.5*IQR\n",
    "sup_lim = Q3 + 1.5*IQR\n",
    "\n",
    "inf_lim, sup_lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_outliers = df_all_no_duplicates['clnt_age'][(df_all_no_duplicates['clnt_age'] < inf_lim) | (df_all_no_duplicates['clnt_age'] > sup_lim) ]\n",
    "\n",
    "# age_outliers         # THERE ARE NO OUTLIERS ON Client Age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VER SI SON NUEVOS O ANTIGUOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_no_duplicates[\"clnt_tenure_mnth\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"clnt_tenure_mnth\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df_all_no_duplicates['clnt_tenure_mnth'])            # Seguimos teniendo outliers pero dar una segunda pasada es contraproducente en principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_all_no_duplicates['clnt_tenure_mnth'].quantile(0.25)\n",
    "Q3 = df_all_no_duplicates['clnt_tenure_mnth'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "inf_lim = Q1 - 1.5*IQR\n",
    "sup_lim = Q3 + 1.5*IQR\n",
    "\n",
    "inf_lim, sup_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIA 3  \n",
    "## Calcular % de compras con éxito en cada web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_max_compras = df_all.groupby([\"client_id\", \"visit_id\", \"variation\"])[\"date_time\"].idxmax()\n",
    "df_id_max_compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ambos_portales = df_all.loc[df_id_max_compras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control = df_ambos_portales[df_ambos_portales[\"variation\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control_step_0 = df_control[df_control[\"process_step\"] == 0]\n",
    "df_control_step_1 = df_control[df_control[\"process_step\"] == 1]\n",
    "df_control_step_2 = df_control[df_control[\"process_step\"] == 2]\n",
    "df_control_step_3 = df_control[df_control[\"process_step\"] == 3]\n",
    "df_control_step_4 = df_control[df_control[\"process_step\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_step0_control = 100 * df_control_step_0.shape[0] / df_control.shape[0]\n",
    "perc_step1_control = 100 * df_control_step_1.shape[0] / df_control.shape[0]\n",
    "perc_step2_control = 100 * df_control_step_2.shape[0] / df_control.shape[0]\n",
    "perc_step3_control = 100 * df_control_step_3.shape[0] / df_control.shape[0]\n",
    "\n",
    "perc_confirms_control = 100 * df_control_step_4.shape[0] / df_control.shape[0]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "El % de gente que se quedó en el INICIO en la web de CONTROL es de {perc_step0_control}\n",
    "El % de gente que se quedó en el PASO 1 en la web de CONTROL es de {perc_step1_control}\n",
    "El % de gente que se quedó en el PASO 2 en la web de CONTROL es de {perc_step2_control}\n",
    "El % de gente que se quedó en el PASO 3 en la web de CONTROL es de {perc_step3_control}\n",
    "El % de gente que CONFIRMÓ la compra en la web de CONTROL es de {perc_confirms_control}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_ambos_portales[df_ambos_portales[\"variation\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_step_0 = df_test[df_test[\"process_step\"] == 0]\n",
    "df_test_step_1 = df_test[df_test[\"process_step\"] == 1]\n",
    "df_test_step_2 = df_test[df_test[\"process_step\"] == 2]\n",
    "df_test_step_3 = df_test[df_test[\"process_step\"] == 3]\n",
    "df_test_step_4 = df_test[df_test[\"process_step\"] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_step0_test = 100 * df_test_step_0.shape[0] / df_test.shape[0]\n",
    "perc_step1_test = 100 * df_test_step_1.shape[0] / df_test.shape[0]\n",
    "perc_step2_test = 100 * df_test_step_2.shape[0] / df_test.shape[0]\n",
    "perc_step3_test = 100 * df_test_step_3.shape[0] / df_test.shape[0]\n",
    "\n",
    "perc_confirms_test = 100 * df_test_step_4.shape[0] / df_test.shape[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "El % de gente que se quedó en el INICIO en la web de TEST es de {perc_step0_test}\n",
    "El % de gente que se quedó en el PASO 1 en la web de TEST es de {perc_step1_test}\n",
    "El % de gente que se quedó en el PASO 2 en la web de TEST es de {perc_step2_test}\n",
    "El % de gente que se quedó en el PASO 3 en la web de TEST es de {perc_step3_test}\n",
    "El % de gente que CONFIRMÓ la compra en la web de TEST es de {perc_confirms_test}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contar y normalizar las ocurrencias de process_step en cada DataFrame\n",
    "control_counts = df_control['process_step'].value_counts(normalize=True).sort_index() * 100  \n",
    "# normalize=True me permite obtener la proporcion. Si no, obtengo datos absolutos y al ser los dos df con distintos número de valores no puedo compararlos \n",
    "test_counts = df_test['process_step'].value_counts(normalize=True).sort_index() * 100  \n",
    "\n",
    "# Crear un DataFrame para facilitar la comparación\n",
    "counts_df = pd.DataFrame({'Control (%)': control_counts, 'Test (%)': test_counts})\n",
    "\n",
    "# Graficar\n",
    "counts_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparación Proporcional de process_step entre Control y Test')\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Porcentaje de Ocurrencias (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Grupo')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que df_control y df_test son tus DataFrames con los datos\n",
    "\n",
    "# Contar y normalizar las ocurrencias de process_step en cada DataFrame\n",
    "control_counts = df_control['process_step'].value_counts(normalize=True).sort_index() * 100\n",
    "test_counts = df_test['process_step'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Crear un DataFrame para facilitar la comparación\n",
    "counts_df = pd.DataFrame({'Control (%)': control_counts, 'Test (%)': test_counts})\n",
    "\n",
    "# Invertir el orden de los índices para calcular correctamente la probabilidad acumulada desde el paso 0\n",
    "counts_df = counts_df[::-1]\n",
    "\n",
    "# Calcular la probabilidad acumulada\n",
    "counts_df['Control Acumulado (%)'] = counts_df['Control (%)'].cumsum()\n",
    "counts_df['Test Acumulado (%)'] = counts_df['Test (%)'].cumsum()\n",
    "\n",
    "# Invertir de nuevo para graficar correctamente\n",
    "counts_df = counts_df[::-1]\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(counts_df.index, counts_df['Control Acumulado (%)'], marker='o', label='Control Acumulado (%)', color='blue')\n",
    "plt.plot(counts_df.index, counts_df['Test Acumulado (%)'], marker='o', label='Test Acumulado (%)', color='orange')\n",
    "plt.title('Probabilidad Acumulada de process_step entre Control y Test')\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Probabilidad Acumulada (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Grupo')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIA 4 -> TEST DE HIPÓTESIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: df_control == df_test\n",
    "# H1: df_control != df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Supongamos que df_control y df_test son tus DataFrames\n",
    "# Contar el número de clientes que completaron la compra (step 4)\n",
    "control_complete = df_control[df_control[\"process_step\"] == 4].shape[0]\n",
    "test_complete = df_test[df_test[\"process_step\"] == 4].shape[0]\n",
    "\n",
    "# Contar el número total de clientes en cada grupo\n",
    "control_total = df_control.shape[0]\n",
    "test_total = df_test.shape[0]\n",
    "\n",
    "# Calcular las tasas de completación\n",
    "control_rate = control_complete / control_total\n",
    "test_rate = test_complete / test_total\n",
    "\n",
    "# Imprimir las tasas de completación\n",
    "print(f\"Tasa de completación del grupo de control: {control_rate:.2%}\")\n",
    "print(f\"Tasa de completación del grupo de test: {test_rate:.2%}\")\n",
    "\n",
    "# Realizar el test de dos proporciones (Z-test)\n",
    "count = [control_complete, test_complete]\n",
    "nobs = [control_total, test_total]\n",
    "stat, p_value = proportions_ztest(count, nobs, alternative='smaller')\n",
    "\n",
    "# Imprimir los resultados del test\n",
    "print(f\"Estadístico Z: {stat:.4f}\")\n",
    "print(f\"P-valor: {p_value:.4f}\")\n",
    "\n",
    "# Interpretar el resultado\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"La prueba de control es inferior a la de test\")\n",
    "else:\n",
    "    print(\"La diferencia en la tasa de completación no es estadísticamente significativa (no podemos rechazar H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Contar el número de clientes que completaron la compra (step 4)\n",
    "control_complete = df_control[df_control[\"process_step\"] == 4].shape[0]\n",
    "test_complete = df_test[df_test[\"process_step\"] == 4].shape[0]\n",
    "\n",
    "# Contar el número de clientes que no completaron la compra\n",
    "control_incomplete = df_control.shape[0] - control_complete\n",
    "test_incomplete = df_test.shape[0] - test_complete\n",
    "\n",
    "# Crear la tabla de contingencia\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Complete': [control_complete, test_complete],\n",
    "    'Incomplete': [control_incomplete, test_incomplete]\n",
    "}, index=['Control', 'Test'])\n",
    "\n",
    "# Aplicar el test chi-cuadrado\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Imprimir los resultados del test\n",
    "print(f\"Chi-cuadrado: {chi2:.4f}\")\n",
    "print(f\"P-valor: {p:.4f}\")\n",
    "\n",
    "# Interpretar el resultado\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"La diferencia en la tasa de completación es estadísticamente significativa (rechazamos H0).\")\n",
    "else:\n",
    "    print(\"La diferencia en la tasa de completación no es estadísticamente significativa (no podemos rechazar H0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar el número de clientes que completaron la compra (step 4)\n",
    "control_complete = df_control[df_control[\"process_step\"] == 4].shape[0]\n",
    "test_complete = df_test[df_test[\"process_step\"] == 4].shape[0]\n",
    "\n",
    "# Contar el número de clientes en cada grupo\n",
    "n_control = df_control.shape[0]\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# Calcular la tasa de completación\n",
    "control_completion_rate = control_complete / n_control\n",
    "test_completion_rate = test_complete / n_test\n",
    "\n",
    "print(f\"Tasa de completación del grupo Control: {control_completion_rate:.4f}\")\n",
    "print(f\"Tasa de completación del grupo Test: {test_completion_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el aumento relativo\n",
    "increase_observed = (test_completion_rate - control_completion_rate) / control_completion_rate * 100\n",
    "\n",
    "print(f\"Aumento observado en la tasa de completación: {increase_observed:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5  # 5%\n",
    "\n",
    "if increase_observed >= threshold:\n",
    "    print(\"El aumento observado cumple con el umbral del 5%.\")\n",
    "else:\n",
    "    print(\"El aumento observado no cumple con el umbral del 5%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_result_gen_var = pd.crosstab(df_ambos_portales[\"gendr\"], df_ambos_portales[\"variation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_result_gen_var\n",
    "\n",
    "crosstab_result_gen_var.plot(kind=\"bar\", stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 = There is no association between 'variation' and 'gendr'  --- Null Hypothesis p>=0.05\n",
    "# H1 = There is an association between 'variation' and 'gendr'  --- Alternative Hypothesis p<0.05\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_statistic, chi2_p_value, _, _ = chi2_contingency(crosstab_result_gen_var)\n",
    "\n",
    "chi2_statistic, chi2_p_value\n",
    "\n",
    "if chi2_p_value < 0.05:\n",
    "    print(\"We reject the null hypothesis. There is an association between 'variation' and 'gendr'.\")\n",
    "else:\n",
    "    print(\"There are no enough evidence that there is an association between 'variation' and 'gendr'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_result_gen_bal = pd.crosstab(df_ambos_portales[\"gendr\"], df_ambos_portales[\"bal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 = There is no association between 'balance' and 'gendr'  --- Null Hypothesis p>=0.05\n",
    "# H1 = There is an association between 'balance' and 'gendr'  --- Alternative Hypothesis p<0.05\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_statistic, chi2_p_value, _, _ = chi2_contingency(crosstab_result_gen_bal)\n",
    "\n",
    "chi2_statistic, chi2_p_value\n",
    "\n",
    "if chi2_p_value < 0.05:\n",
    "    print(\"We reject the null hypothesis. There is an association between 'balance' and 'gendr'.\")\n",
    "else:\n",
    "    print(\"There are no enough evidence that there is an association between 'balance' and 'gendr'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are Male balance mean greater than Female or Unknowns balance mean?\n",
    "\n",
    "df_female = df_ambos_portales[(df_ambos_portales[\"gendr\"] == \"F\")][\"bal\"]\n",
    "df_male = df_ambos_portales[df_ambos_portales[\"gendr\"] == \"M\"][\"bal\"]\n",
    "df_unknown = df_ambos_portales[df_ambos_portales[\"gendr\"] == \"U\"][\"bal\"]\n",
    "\n",
    "p_value_MvF = st.ttest_ind(df_male, df_female, alternative=\"greater\")\n",
    "p_value_FvU = st.ttest_ind(df_female, df_unknown, alternative=\"greater\")\n",
    "\n",
    "if p_value_MvF[1] < 0.05 and p_value_FvU[1] < 0.05:\n",
    "    print(\"We reject the null hypothesis from both hypotheses. Males balance mean are greater than females or unknowns balance mean.\")\n",
    "else:\n",
    "    print(\"There are no enough evidence to refuse the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS THE AVERAGE BALANCE DIFFERENT DEPPENDING ON AGE?\n",
    "\n",
    "crosstab_result_age_bal = pd.crosstab(df_ambos_portales[\"clnt_age\"], df_ambos_portales[\"bal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 = There is no association between 'clnt_age' and 'bal'  --- Null Hypothesis p>=0.05\n",
    "# H1 = There is an association between 'clnt_age' and 'bal'  --- Alternative Hypothesis p<0.05\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_statistic, chi2_p_value, _, _ = chi2_contingency(crosstab_result_gen_bal)\n",
    "\n",
    "chi2_statistic, chi2_p_value\n",
    "\n",
    "if chi2_p_value < 0.05:\n",
    "    print(\"We reject the null hypothesis. There is an association between 'clnt_age' and 'gendr'.\")\n",
    "else:\n",
    "    print(\"There are no enough evidence that there is an association between 'clnt_age' and 'gendr'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIAS 3 Y 4 DANI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las compras confirmadas (process_step = 4)\n",
    "df_all['num_compras'] = df_all['process_step'].apply(lambda x: 1 if x == 4 else 0)          # Creamos columna número de compras\n",
    "\n",
    "# Calcular ICU\n",
    "df_all['ICU'] = (df_all['logons_6_mnth'] + df_all['calls_6_mnth'] + df_all['num_compras']) / (df_all['clnt_tenure_mnth'] + df_all['num_accts'])\n",
    "\n",
    "# Comparar el rendimiento entre portales (1 para nuevo, 0 para antiguo)\n",
    "kpi_portal_antiguo = df_all[df_all['variation'] == 0]['ICU'].mean()\n",
    "kpi_portal_nuevo = df_all[df_all['variation'] == 1]['ICU'].mean()\n",
    "\n",
    "print(\"ICU - Portal Antiguo:\", kpi_portal_antiguo)\n",
    "print(\"ICU - Portal Nuevo:\", kpi_portal_nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular total de compras y visitas para cada portal\n",
    "total_visitas_antiguo = df_all[df_all['variation'] == 0]['visit_id'].nunique()\n",
    "total_compras_antiguo = df_all[(df_all['variation'] == 0) & (df_all['process_step'] == 4)]['visit_id'].nunique()\n",
    "\n",
    "total_visitas_nuevo = df_all[df_all['variation'] == 1]['visit_id'].nunique()\n",
    "total_compras_nuevo = df_all[(df_all['variation'] == 1) & (df_all['process_step'] == 4)]['visit_id'].nunique()\n",
    "\n",
    "tcc_antiguo = total_compras_antiguo / total_visitas_antiguo\n",
    "tcc_nuevo = total_compras_nuevo / total_visitas_nuevo\n",
    "\n",
    "print(\"TCC - Portal Antiguo:\", tcc_antiguo)\n",
    "print(\"TCC - Portal Nuevo:\", tcc_nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Crea un arreglo con las etiquetas del eje x\n",
    "x = np.arange(2)\n",
    "\n",
    "# Crea un gráfico de barras\n",
    "plt.bar(x - 0.2, [total_compras_antiguo, total_compras_nuevo], label='Compras', width=0.4)\n",
    "plt.bar(x + 0.2, [total_visitas_antiguo, total_visitas_nuevo], label='Visitas', width=0.4)\n",
    "\n",
    "# Agrega un título y etiquetas a los ejes\n",
    "plt.title('Total de Compras y Visitas')\n",
    "plt.xlabel('Período')\n",
    "plt.ylabel('Cantidad')\n",
    "\n",
    "# Agrega etiquetas al eje x\n",
    "plt.xticks(x, ['Antiguo', 'Nuevo'])\n",
    "\n",
    "# Agrega una leyenda\n",
    "plt.legend()\n",
    "\n",
    "# Muestra el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compras_antiguo, total_compras_nuevo,total_visitas_antiguo,total_visitas_nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular duración de las visitas\n",
    "df_all['visit_duration'] = df_all.groupby('visit_id')['date_time'].transform(lambda x: (x.max() - x.min()).total_seconds())\n",
    "\n",
    "# Filtrar las visitas con compras\n",
    "compras_antiguo = df_all[(df_all['variation'] == 0) & (df_all['process_step'] == 4)]\n",
    "compras_nuevo = df_all[(df_all['variation'] == 1) & (df_all['process_step'] == 4)]\n",
    "\n",
    "dpc_antiguo = compras_antiguo['visit_duration'].mean()\n",
    "dpc_nuevo = compras_nuevo['visit_duration'].mean()\n",
    "\n",
    "print(\"DPC - Portal Antiguo:\", dpc_antiguo)\n",
    "print(\"DPC - Portal Nuevo:\", dpc_nuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarnos de que 'date_time' esté en formato datetime\n",
    "df_all['date_time'] = pd.to_datetime(df_all['date_time'])\n",
    "\n",
    "# Extraer solo la parte de la hora (HH:MM:SS) si es necesario\n",
    "df_all['time'] = df_all['date_time'].dt.time\n",
    "\n",
    "# Ordenar los datos por client_id, visit_id y process_step\n",
    "df_sorted = df_all.sort_values(by=['client_id', 'visit_id', 'process_step'])\n",
    "\n",
    "# Calcular el tiempo entre pasos, ignorando el primer paso\n",
    "df_sorted['time_diff'] = df_sorted.groupby(['client_id', 'visit_id'])['date_time'].diff()\n",
    "\n",
    "# Filtrar los process_step > 0 para evitar valores incorrectos para el primer paso\n",
    "df_filtered = df_sorted[df_sorted['process_step'] > 0]\n",
    "\n",
    "# Filtrar los datos por portales\n",
    "df_antiguo = df_filtered[df_filtered['variation'] == 0]\n",
    "df_nuevo = df_filtered[df_filtered['variation'] == 1]\n",
    "\n",
    "# Calcular el tiempo promedio entre pasos para cada portal\n",
    "avg_time_diff_antiguo = df_antiguo.groupby('process_step')['time_diff'].mean()\n",
    "avg_time_diff_nuevo = df_nuevo.groupby('process_step')['time_diff'].mean()\n",
    "\n",
    "# Crear un DataFrame para las comparaciones\n",
    "df_avg_time_diff = pd.DataFrame({\n",
    "    'Control': avg_time_diff_antiguo.dt.total_seconds(),\n",
    "    'Test': avg_time_diff_nuevo.dt.total_seconds()\n",
    "})\n",
    "\n",
    "# Gráfico de barras\n",
    "df_avg_time_diff.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Tiempo Promedio Entre Pasos para Cada Portal')\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Tiempo Promedio (segundos)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Filtrar los process_step > 0 para evitar valores incorrectos para el primer paso\n",
    "df_filtered = df_sorted[df_sorted['process_step'] > 0].copy()\n",
    "\n",
    "# Calcular el tiempo acumulado por visita y por paso\n",
    "# Primero, calculamos el tiempo del paso anterior (equivalente a lag)\n",
    "df_filtered['time_diff_shifted'] = df_filtered.groupby(['client_id', 'visit_id'])['time_diff'].shift()\n",
    "\n",
    "# Luego, sumamos el tiempo acumulado de los pasos anteriores para cada cliente y visita\n",
    "df_filtered['time_accumulated'] = df_filtered.groupby(['client_id', 'visit_id'])['time_diff_shifted'].cumsum().fillna(pd.Timedelta(seconds=0))\n",
    "\n",
    "# Filtrar nuevamente por portales\n",
    "df_control_accum = df_filtered[df_filtered['variation'] == 0]\n",
    "df_test_accum = df_filtered[df_filtered['variation'] == 1]\n",
    "\n",
    "# Calcular el tiempo acumulado promedio por cada paso en segundos\n",
    "acc_time_antiguo = df_control_accum.groupby('process_step')['time_accumulated'].mean().dt.total_seconds()\n",
    "acc_time_nuevo = df_test_accum.groupby('process_step')['time_accumulated'].mean().dt.total_seconds()\n",
    "\n",
    "# Crear un DataFrame para las comparaciones\n",
    "df_acc_time = pd.DataFrame({\n",
    "    'Portal Antiguo': acc_time_antiguo,\n",
    "    'Portal Nuevo': acc_time_nuevo\n",
    "})\n",
    "\n",
    "# Eliminar filas con valores NaN en cualquier columna (si las hay)\n",
    "df_acc_time.dropna(inplace=True)\n",
    "\n",
    "# Crear un DataFrame con el punto (0, 0)\n",
    "df_zero_point = pd.DataFrame({\n",
    "    'Portal Antiguo': [0],\n",
    "    'Portal Nuevo': [0]\n",
    "}, index=[0])\n",
    "\n",
    "# Concatenar el punto (0, 0) con los datos acumulados\n",
    "df_acc_time = pd.concat([df_zero_point, df_acc_time])\n",
    "\n",
    "# Crear el gráfico de líneas, incluyendo el punto (0, 0)\n",
    "df_acc_time.plot(kind='line', figsize=(10, 6), marker='o')\n",
    "plt.title('Tiempo Acumulado Promedio por Proceso para Cada Portal (Incluyendo Punto 0)')\n",
    "plt.xlabel('Process Step')\n",
    "plt.ylabel('Tiempo Acumulado (segundos)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit duration depends on variation?\n",
    "\n",
    "control_times = df_control_accum['time_accumulated'].dt.total_seconds()\n",
    "test_times = df_test_accum['time_accumulated'].dt.total_seconds()\n",
    "\n",
    "st.ttest_ind(test_times, control_times, alternative=\"greater\")\n",
    "\n",
    "# We can confimr that the test time is greater than the control time with a 95% confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIA 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ambos_portales['confirmed_purchase'] = df_ambos_portales['process_step'].apply(lambda x: 1 if x == 4 else 0)\n",
    "crosstab_result_age_confirm = pd.crosstab(df_ambos_portales['clnt_age'], df_ambos_portales['confirmed_purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 = There is no association between 'balance' and 'gendr'  --- Null Hypothesis p>=0.05\n",
    "# H1 = There is an association between 'balance' and 'gendr'  --- Alternative Hypothesis p<0.05\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "chi2_statistic, chi2_p_value, _, _ = chi2_contingency(crosstab_result_age_confirm)\n",
    "\n",
    "chi2_statistic, chi2_p_value\n",
    "\n",
    "if chi2_p_value < 0.05:\n",
    "    print(\"We reject the null hypothesis. There is an association between 'age' and confirming purchase.\")\n",
    "else:\n",
    "    print(\"There is no significant evidence to suggest an association between 'age' and confirming purchase.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
